---
title: "Lesson 2 - Statistics"
author: "Geoffrey Millard"
date: "July 17, 2019"
self_contained: yes
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Picking up where we left off
For this lesson, we are going to use data from the United States Geological Survey.  The same data we pulled from NWIS towards the end of lesson 1. (repeated here)


Switch to web Browser and go to NWIS Mapper: https://maps.waterdata.usgs.gov/mapper/index.html
Find Honnedaga Lake gauges and sampling sites

```{r set up workspace, warning=F, message=FALSE}
library(tidyverse)

#USGS and EPA dataset retrieval package
library(dataRetrieval) 
#click on "Packages" tab to see functions available in the dataRetrival package

#these are the USGS sites upstream from Honnedaga Lake, NY that we found in the browser
sites <- c('0134277112', '0134277114')

#detailed location information about the site
# readNWISsite(c('0134277112', '0134277114'))

#whatNWISdata displays the datasets available at each site
available <- whatNWISdata(siteNumber=c('0134277112', '0134277114'))
actual <- readNWISpCode(parameterCd = available$parm_cd) #interpret parameter codes
want <- c('00681', '00945', '50287', '50285') #we want: DOC, DSO4, DHg and DMeHg
data <- readNWISqw(siteNumbers = sites, parameterCd=want) #get the data
codes <- readNWISpCode(parameterCd = unique(data$parm_cd)) #confirms that we got what we wanted, shows UNITS!
View(data)
```
## What do we know?

Two sites and four parameters.  How can we quickly generate some summary statistics?
Piping!

```{r}
#first we need to name the parameter codes, so that we don't forget which code is which
data <- data %>% mutate(Analyte = recode(parm_cd, 
                         '00681'= "DOC", 
                         '00945' = "SO4", 
                         '50287' = "Hg", 
                         '50285' = "MeHg"))

data <- data %>% mutate(Site = recode(site_no, 
                         '0134277112' = "Reference", 
                         '0134277114' = "Treated"))


#summary stats
data %>% group_by(Site, Analyte) %>% summarize(average=mean(result_va, na.rm = T))

```

## Yea, so?

We can do this for more than one summary statistic at a time!

```{r summarize a bunch of stuff}
data %>% 
  group_by(Site, Analyte) %>% 
  summarize(average=mean(result_va), 
            stdv=sd(result_va, na.rm=T),
            max=max(result_va, na.rm=T), 
            min=min(result_va, na.rm=T), 
            n = n())
```

Awesome, but what if I wanted to look at this through time?  Monthly or annual?  We can use functions that are part of the lubridate package to quickly and easily manipulate dates.

```{r summarize by date, warning=F, message=F, results=F}
library(lubridate) # install.packages("lubridate") if needed
class(data$sample_dt) # we can see R recognizes this as a date
```

Using Lubridate, we can look at the years, months, days, etc. separately from other date information.

```{r}
# we can look at the years, or months separately from everything else
head(year(data$sample_dt))
head(month(data$sample_dt))
```

Using this package, we can generate summary statistics for each month in each year.

```{r}

summary <- data %>% 
  group_by(Site, Analyte, year(sample_dt), month(sample_dt)) %>% 
  summarize(average=mean(result_va), 
            stdv=sd(result_va, na.rm=T),
            max=max(result_va, na.rm=T), 
            min=min(result_va, na.rm=T), 
            n = n())
summary
```

Custom groupings are a little more complicated, but can be done.
#pre and post treatment data (Creating categorical variables from continuous variables)
We can use the values in the sample_dt column to generate catagorical data.  In this case, we can split everything into three groups: pre-treatment (Group 1), post-treatment (Group 2), long-term post-treatment (Group 3). Once we have the catagories, we can again generate summary statistics.

```{r custom groups}
data$Treatment[ data$sample_dt < as.Date('2013-10-1')  ] <- 1
data$Treatment[ data$sample_dt >= as.Date('2013-10-1') ] <- 2
data$Treatment[ data$sample_dt > as.Date('2014-02-28') ] <- 3

data %>% 
  group_by(Site, Analyte, Treatment) %>% 
  summarize(average=mean(result_va, na.rm = T), 
            stdv=sd(result_va, na.rm=T),
            max=max(result_va, na.rm=T), 
            min=min(result_va, na.rm=T), 
            n = n())
```

This makes it really easy to break your data down in a variety of ways relatively quickly.


## Linear models and correlations

We have some descriptive stats, but we want to see if there are any relationships in our data.  Base-R has some pretty good tools for this, but they don't work very well with data in long format, so we need to switch to a wide format.

```{r cor and lm}
simple <- data %>% select(Site, Date = sample_dt, Time = sample_tm, Analyte, Result = result_va, Treatment)
simple <- data.frame(Site=data$Site, Date = data$sample_dt, Time = data$sample_tm, Analyte=data$Analyte, Result = data$result_va, Treatment=data$Treatment)
```

We need both date and time above, or the spread will not work (multiple samples were collected on the same day after all).

Now we can use some dplyr functions (part of tidyverse package) to reshape our dataset.

```{r}
wide <- simple %>% spread(key = Analyte, value = Result)
head(wide)
```

This will allow us to quickly generate a correlation.  Just make sure you are directing R to use the correct set of data and the correct method.

```{r}
cor(x= wide$DOC, y= wide$Hg)
cor(x= wide$DOC, y= wide$Hg, use = 'complete.obs') # default method is a Pearson correlation
cor(x= wide$DOC, y= wide$Hg, use = 'complete.obs', method = 'kendall')
```

It would also be helpful to quickly visualize our data.  We can do this using basic R functions, but we will cover that in greater detail in lesson 3.

```{r}
#Visualize data in base plot
plot(wide$DOC, wide$Hg, xlab = "DOC in mg/L", ylab = "Dissolved Hg in ng/L")
```

#create a linear regression model and visualize residuals
I don't use a lot of correlations in my research, I tend to look for linear regressions.  R can handle this using the `lm()` function.

```{r}
reg1 <- lm(data=wide, Hg ~ DOC) #create linear regression model
reg1 #this is the output of the linear model object
```

This is great, but it doesn't actually give us enough information to draw any sort of conclusion.  If we pair this with the `summary()` function, we get a better idea of what is happening.

```{r}
summary(reg1)
```
We wanted to see if DOC concentrations predict Hg concentrations (Formula = Hg ~ DOC)
Residuals are mostly, but not completely symmetrical -- might have a left skew, but that be because we have 6 treatment groups in the data set

Estimate of intercept has a significant p-value, saying that it is significantly different from zero, but in real life, there are no negative Hg concentrations, so it is not valuable information

The estimate of "DOC" is actually the slope of the line, meaning that for each unit of DOC in mg/L, the concentration of Hg is expected to increase about a half of 1 ng/L. The *** and p-value < 0.05 indicate that the slope is significantly different from zero.

The R-squared values are a measure of "fit."  If all the data points fell on the regression line, R-squared = 1.

### Diagnostic plots

If we want to look at more diagnostic information, the base R plotting function can again be helpful.  If we change the parameters of the plotting window, we can see all four default diagnostic figures at the same time.

```{r}
par(mfrow=c(2,2))
plot(reg1)
par(mfrow=c(1,1))
```

#plot the model on the data

We can add the linear regression to the earlier plot by adding the `abline()` function after the `plot()` function.  We can also make adjustments to the color so it is easier to see.

```{r}
plot(wide$DOC, wide$Hg, xlab = "DOC in mg/L", ylab = "Dissolved Hg in ng/L")
abline(reg1, lty = 2, col = "cornflowerblue")

```


# Pairwise comparisons

With different treatment groups, a common test is a t-test or a Tukey test.  Let's set up a couple of these tests with our data looking at the distribution (can we use parametric tests) and some groupings.

First we need to set up the class(Factors).

```{r statistical tests}
wide$Site <- as.factor(wide$Site)
wide$Treatment <- as.factor(wide$Treatment)
```

## Check for normality
Once we have these factors, we can look at a histogram, or use a Shapiro-Wilk test to check for normality.

```{r}
hist(wide$DOC)

shapiro.test(wide$DOC) # null hypothesis: data is normal
shapiro.test(wide$Hg)
shapiro.test(wide$DOC[wide$Site == "Reference"])
```

Neither dataset appears to be normally distributed (pvalue is less than 0.05), but for the interest of this excercise, lets look at both some parametric and non-parametric tests.

## Non-parametric example

```{r}
#non-parametic example
kruskal.test(data = wide, DOC ~ Site) #like an anova, testing for differences in DOC concentration by Site
kruskal.test(data = wide, DOC ~ Treatment) #testing for differences in DOC conc by Treatment
```

## Parametic example
We can use the `aov()` function to look at Tukey's Honest Square Difference, or a general ANOVA test.

```{r}
aov1 <- aov(data = wide, DOC ~ Site + Treatment + Site*Treatment)
summary(aov1)
TukeyHSD(aov1)
anova(aov1)
```

This example doesn't have any interaction between the site and treatment level. If it did, we could set up a contrast matrix.  This requires having a unique identifier for each level of treatment.

```{r contrasts, warning=F, message=F}
library(multcomp)
wide$TMT <- interaction(wide$Site, wide$Treatment)
aov2 <- aov(data=wide, DOC ~ TMT)
summary(aov2)
```

Notice the difference between the `summary()` of aov1 and aov2.  You need to aov1 results to justify building a contrast matrix for aov2.

Speaking of a contrast matrix, lets create one.  The order of the factors in your treatment (where we used the `interaction()` function), is the order that the contrast matrix will be constructed in.

```{r}
cntrMat <- rbind(
  "Main effect of Site"=c(1, 1, 1, -1, -1, -1),
  "(ref1-treat1)-(ref3-treat3)"=c(1, 0, -1, -1, 0, 1),
  "(ref1-treat1)-(ref2-treat2)"=c(1,  -1, 0, -1, 1, 0), 
  "(ref2-treat2)-(ref3-treat3)"=c(0, 1, -1, 0, -1, 1)
  ) 

# The contrasts above could also be labeled as differences between time periods (eg. (ref1-ref3)-(treat1-treat3)).  Mathematically, these are the same.

#general linear hypothesis
glht(aov2, linfct=mcp(TMT=cntrMat), alternative="two.sided") # alternative='two.sided' is the default

summary(glht(aov2, linfct=mcp(TMT=cntrMat)), test=adjusted("none"))  
```

The `mcp()` function {Multiple ComParisons} changes the hypothesis so that you are testing if the difference between the groups is zero, instead of the difference between slopes.

Notice that the summary generates a table similar to what we have seen before with estimates of the difference between the group means and a p-value.  This is not very easy to share with colleages, so your could use Rmarkdown (like we have for these lessons) to create an easy to read table like this one:


```{r pretty table, warning=F, message=F}
library(kableExtra)
library(broom)
kable(tidy(summary(glht(aov2, linfct=mcp(TMT=cntrMat), alternative="two.sided"), test=adjusted("none"))), digits = 3) %>% kable_styling(bootstrap_options = c("striped", "hover", 'responsive'), full_width = F)
```

